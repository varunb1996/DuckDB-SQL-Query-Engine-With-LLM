{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a63d28-6d82-43f4-ba56-03b76259fc0c",
   "metadata": {},
   "source": [
    "BUILDING A RAG APPLICATION WITH DUCKDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c30ad4-1117-4e4a-ac78-f152e3ef2d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install duckdb\n",
    "%pip install llama-index\n",
    "%pip install llama-index-vector-stores-duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7f07a5-398c-4c7c-acbb-d49dd3cc6094",
   "metadata": {},
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.duckdb import DuckDBVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2453316a-df09-4479-8b80-2c082f4d2f63",
   "metadata": {},
   "source": [
    "Setting up GPT-4o and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b7265-bf87-4d75-aff0-94fea3d528db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\",api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0630fe-baf7-4f6a-8a9b-86db80e37e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf56f38-5b91-4973-9807-96f22d4f3d5f",
   "metadata": {},
   "source": [
    "Using DuckDB as a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e3dbe-2455-43f0-a408-a0e36182e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"Data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a89af1-97be-4d88-aea8-69956dde2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = DuckDBVectorStore(database_name = \"datacamp.duckdb\",table_name = \"blog\",persist_dir=\"./\", embed_dim=1536)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71372790-d9a4-4bd4-8e1b-0ba492cb30e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "con = duckdb.connect(\"datacamp.duckdb\")\n",
    "\n",
    "con.execute(\"SHOW ALL TABLES\").fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f62ade-c44e-4812-89fa-cab4781e7902",
   "metadata": {},
   "source": [
    "Creating a RAG Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14263c29-c8fd-418a-a774-7ddd7a812d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Who wrote 'GitHub Actions and MakeFile: A Hands-on Introduction'?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e7ebd-6e9a-45d4-9f68-5a8fe16ee52f",
   "metadata": {},
   "source": [
    "The answer is correct - The author of \"GitHub Actions and MakeFile: A Hands-on Introduction\" is Abid Ali Awan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59025109-3afb-446c-867d-b0fa510722d0",
   "metadata": {},
   "source": [
    "Creating a RAG chatbot with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e75f58-8ed7-4479-a1c2-4296561f9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.chat_engine import CondensePlusContextChatEngine\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n",
    "\n",
    "chat_engine = CondensePlusContextChatEngine.from_defaults(\n",
    "    index.as_retriever(),\n",
    "    memory=memory,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "response = chat_engine.chat(\n",
    "    \"What is the easiest way of finetuning the Llama 3 model? Please provide step-by-step instructions.\"\n",
    ")\n",
    "\n",
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7225512f-2799-4674-afe7-6b17861a0bb3",
   "metadata": {},
   "source": [
    "We asked the chat engine how to fine-tune the Llama 3 model, and it used the vector store to give a highly accurate answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf2ac9-3411-49a8-b2bd-51199ac477bd",
   "metadata": {},
   "source": [
    "To check if the memory buffer is working correctly, we will ask a follow-up question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a53aba-1190-4a28-ae68-ab9cf392b5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\n",
    "    \"Could you please provide more details about the Post Fine-Tuning Steps?\"\n",
    ")\n",
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c7df7-6957-447c-80c2-29332d1de171",
   "metadata": {},
   "source": [
    "The chat engine remembered the previous conversation and responded accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d144b-e347-4c03-9924-57ecf7773ae2",
   "metadata": {},
   "source": [
    "BUILDING A DUCKDB SQL QUERY ENGINE USING AN LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8270be4b-481b-42d7-9a82-81c6a798443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install duckdb-engine -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d674e4-53b2-4f9f-9e66-81e91dbd37f0",
   "metadata": {},
   "source": [
    "Loading the DuckDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5316cd-9b67-46b9-a5bb-f3fb16fb94d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"duckdb:///datacamp.duckdb\")\n",
    "with engine.connect() as connection:\n",
    "    cursor = connection.exec_driver_sql(\"SELECT * FROM bank LIMIT 3\")\n",
    "    print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0eff40-eb1f-4f69-83fe-bcd44a373dd8",
   "metadata": {},
   "source": [
    "[(56, 'housemaid', 'married', 'basic.4y', 'no', 'no', 'no', 'telephone', 'may', 'mon', 261, 1, 999, 0, 'nonexistent', 1.1, 93.994, -36.4, 4.857, 5191.0, 'no'), (57, 'services', 'married', 'high.school', 'unknown', 'no', 'no', 'telephone', 'may', 'mon', 149, 1, 999, 0, 'nonexistent', 1.1, 93.994, -36.4, 4.857, 5191.0, 'no'), (37, 'services', 'married', 'high.school', 'no', 'yes', 'no', 'telephone', 'may', 'mon', 226, 1, 999, 0, 'nonexistent', 1.1, 93.994, -36.4, 4.857, 5191.0, 'no')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42299789-b1b0-4970-a603-737962f99488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SQLDatabase\n",
    "sql_database = SQLDatabase(engine, include_tables=[\"bank\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065a7087-5168-41ff-9efc-ec6efb752ef0",
   "metadata": {},
   "source": [
    "Building the SQL query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa32ad-7e0f-42a7-8d4e-4a203eff8c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
    "\n",
    "query_engine = NLSQLTableQueryEngine(sql_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce4de9-c6c1-4148-9230-2f4d866b40aa",
   "metadata": {},
   "source": [
    "Ask the question from the query engine about the “bank” table in the natural language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620fdf3a-24d8-4f68-8674-7de1f5c35071",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Which is the longest running campaign?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71786154-6103-4eff-9217-405909f7a57f",
   "metadata": {},
   "source": [
    "In response, we will get the answer to your query in natural languages. \n",
    "The longest running campaign in the database has a duration of 4918 days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a40049-d33b-47f1-8aac-2a82b7992078",
   "metadata": {},
   "source": [
    "Asking a complex question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64ff24-9059-4c0e-8f14-ec5cfb0951b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Which type of job has the most housing loan?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4b7c3f-d0d4-4a76-a159-8bf2f3474370",
   "metadata": {},
   "source": [
    "The answer with some additional info :\n",
    "The job type with the most housing loans is 'admin.' with 5559 housing loans. This is followed by 'blue-collar' with 4710 housing loans and 'technician' with 3616 housing loans. Other job types with significant housing loans include 'services', 'management', 'retired', 'entrepreneur', and 'self-employed'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717ec00b-a87c-42e1-8d17-f7599442b5cb",
   "metadata": {},
   "source": [
    "The backend info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a6412a-bc5b-4205-89cb-67b8a231a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93376b66-c3a1-4402-b70d-6e407e5aa2e9",
   "metadata": {},
   "source": [
    "As we can see, GPT-4o first generates the SQL query, runs the query to get the result, and uses the result to generate the response. This multi-step process is achieved through two lines of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb70ad0-0976-498c-b5a6-9ee35baaacac",
   "metadata": {},
   "source": [
    "{'d4ddf03c-337e-4ee6-957a-5fd2cfaa4b1c': {}, 'sql_query': \"SELECT job, COUNT(housing) AS housing_loan_count\\nFROM bank\\nWHERE housing = 'yes'\\nGROUP BY job\\nORDER BY housing_loan_count DESC;\", 'result': [('admin.', 5559), ('blue-collar', 4710), ('technician', 3616), ('services', 2050), ('management', 1490), ('retired', 892), ('entrepreneur', 779), ('self-employed', 740), ('unemployed', 557), ('housemaid', 540), ('student', 471), ('unknown', 172)], 'col_keys': ['job', 'housing_loan_count']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ecf7c-2685-4a28-8ed3-03997e539e6d",
   "metadata": {},
   "source": [
    "Close the Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf26def-c435-4806-bac6-8f4e751ea58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
